# -*- coding: utf-8 -*-
"""code2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZNJ8sjvR_sge1prqWo5XCon6zBTnrfDu

""This is the code where we optimized AUC ROC and other metrics without adding new features.
"""

# -*- coding: utf-8 -*-
"""final code

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jeNgh_xh_UsZMEC62-WZeD0K1BuMRAUF

# Data Preprocessing
"""

import matplotlib
matplotlib.use('Agg')  # Use non-interactive backend
import numpy as np
import pandas as pd
from scipy.stats import shapiro
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier
import xgboost as xgb
from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, f1_score, roc_curve, auc
import matplotlib.pyplot as plt
import seaborn as sns

merged_df= pd.read_csv('/Users/krishnayadav/Documents/test_projects/schizophrenia-journal/data/inital_data.csv')


#copying dataframe
df= merged_df.copy()

#creating new required features
df['Dynamic range of pupil size']= (df['PUPIL_SIZE_MAX']- df['PUPIL_SIZE_MIN'] )/ df['PUPIL_SIZE_MEAN']
df['Pupil size ratio'] = df['PUPIL_SIZE_MAX']/ df['PUPIL_SIZE_MEAN']

#dropping columns
df=df.drop(columns=['pic_3_3' , 'PUPIL_SIZE_MAX' , 'PUPIL_SIZE_MIN', 'PUPIL_SIZE_MEAN'])

# Rename multiple columns
new_column_names = {'calculated_result': 'Fixation_skewness', 'CURRENT_FIX_DURATION': 'Valid Viewing Duration', 'CURRENT_SAC_AVG_VELOCITY': 'Average Saccadic Velocity', 'CURRENT_SAC_AMPLITUDE': 'Total Saccade Amplitude'}
df.rename(columns=new_column_names, inplace=True)


# Assuming 'df' is your DataFrame
missing_values = df.isna().sum()
# Initial missing values check (silent)

df.shape

"""# Creating list which contains all subject's data"""

# Assuming your original DataFrame is named 'df'
num_rows = len(df)
chunk_size = 100
num_dataframes = 65

# Calculate the number of rows to cover the first 65 DataFrames
rows_for_dataframes = chunk_size * num_dataframes

# Create a list to hold the smaller DataFrames
dfs = []

# Split the DataFrame into chunks of 100 rows each for the first 65 DataFrames
for i in range(0, rows_for_dataframes, chunk_size):
    dfs.append(df.iloc[i:i+chunk_size])

df66= df.iloc[6500: 6594]
df67= df.iloc[6594:6694]
df68=df.iloc[6694:6794]
df69=df.iloc[6794:6894]
df70=df.iloc[6894:6994]
dfs.append(df66)
dfs.append(df67)
dfs.append(df68)
dfs.append(df69)
dfs.append(df70)

# List of columns to convert to float
columns_to_convert = ['Average Saccadic Velocity', 'AVERAGE_FIXATION_DURATION', 'AVERAGE_SACCADE_AMPLITUDE']

# Loop through each DataFrame in the 'dfs' list
for df in dfs:
    # Convert specified columns to float while ignoring errors
    for col in columns_to_convert:
        df[col] = pd.to_numeric(df[col], errors='coerce').astype(float)

"""# Replacing missing values"""

#### Assuming your list of DataFrames is named 'dfs'
# Columns you want to analyze
columns_to_analyze = ['AVERAGE_FIXATION_DURATION', 'Fixation_skewness','Dynamic range of pupil size','Pupil size ratio','Valid Viewing Duration','Total Saccade Amplitude','Average Saccadic Velocity', 'AVERAGE_SACCADE_AMPLITUDE']

# Loop through each DataFrame and analyze the specified columns (silent processing)
for i, df in enumerate(dfs):
    for column in columns_to_analyze:
        column_data = df[column]

        # Convert the data to numeric if possible
        column_data_numeric = pd.to_numeric(column_data, errors='coerce')

        # Perform the Shapiro-Wilk test
        stat, p = shapiro(column_data_numeric.dropna())

        alpha = 0.05  # Significance level

        if p > alpha:
            # Replace NaN values with the mean (normal distribution)
            mean_value = column_data_numeric.mean()
            df[column].fillna(mean_value, inplace=True)
        else:
            # Replace NaN values with the median (non-normal distribution)
            median_value = column_data_numeric.median()
            df[column].fillna(median_value, inplace=True)


# adding label as a new column in dfs list
# Assuming your list of DataFrames is named 'dfs'
class_values = [0] * 30 + [1] * 40

for i, df in enumerate(dfs):
    df['class'] = class_values[i]

# Verify no missing values remain (silent check)
# for df in dfs:
#     missing_values = df.isna().sum()

l1=dfs[65]

# Assuming l1 is your DataFrame with 94 rows
# Create a new index ranging from 6500 to 6599
new_index = range(6500, 6600)

# Columns to resample and interpolate
columns_to_resample = ['Valid Viewing Duration', 'Total Saccade Amplitude',
                       'Average Saccadic Velocity', 'AVERAGE_FIXATION_DURATION',
                       'AVERAGE_SACCADE_AMPLITUDE', 'FIXATION_COUNT',
                       'SACCADE_COUNT', 'Fixation_skewness',
                       'Dynamic range of pupil size', 'Pupil size ratio', 'class']

# Create a new DataFrame with the new index
l1_resampled = pd.DataFrame(index=new_index)

# Copy the values from the original l1 DataFrame to l1_resampled for the specified columns
l1_resampled[columns_to_resample] = l1[columns_to_resample]

# Interpolate to fill any NaN values
l1_resampled = l1_resampled.interpolate(method='linear')

# Now l1_resampled will have 100 rows with interpolated values for the specified columns
dfs[65] = l1_resampled

dfs[66].index= range(6600,6700)
dfs[67].index = range(6700, 6800)
dfs[68].index = range(6800, 6900)
dfs[69].index = range(6900, 7000)

def validation_calculation(model, dfs_val):
    ypred_val=[]
    true_labels_val=[]
    for df in dfs_val:
        X_val = df.drop(columns=['class'])
        y_pred_val = model.predict(X_val)

        # True labels
        true_label = df['class'].iloc[0]
        true_labels_val.append(true_label)

        # Count the number of rows predicted as 1 in the test set
        count_1s = sum(y_pred_val)

        # If more than 50 rows are predicted as 1, set ypred for this person as 1, else 0
        if count_1s > 50:
            ypred_val.append(1)
        else:
            ypred_val.append(0)

        # Accuracy calculation
    validation_accuracy = accuracy_score(true_labels_val, ypred_val)
    return validation_accuracy

def training_calculation(model, dfs_train):
    ypred_train=[]
    true_labels_train=[]
    for df in dfs_train:
        X_train = df.drop(columns=['class'])
        y_pred_train = model.predict(X_train)

        # True labels
        true_label = df['class'].iloc[0]
        true_labels_train.append(true_label)

        # Count the number of rows predicted as 1 in the test set
        count_1s = sum(y_pred_train)

        # If more than 50 rows are predicted as 1, set ypred for this person as 1, else 0
        if count_1s > 50:
            ypred_train.append(1)
        else:
            ypred_train.append(0)

        # Accuracy calculation
    training_accuracy = accuracy_score(true_labels_train, ypred_train)
    return training_accuracy

from sklearn.metrics import f1_score
def testing_calculation(model, dfs_test):
    true_labels_test=[]
    ypred=[]
    for df in dfs_test:
        X_test = df.drop(columns=['class'])
        y_pred_test = model.predict(X_test)
        true_label = df['class'].iloc[0]
        true_labels_test.append(true_label)
        count_1s = sum(y_pred_test)
        if count_1s > 50:
            ypred.append(1)
        else:
            ypred.append(0)

    # Confusion matrix
    cm = confusion_matrix(true_labels_test, ypred)
    TN, FP, FN, TP = cm.ravel()
    # Sensitivity calculation
    testing_sensitivity = TP / (TP + FN)
    # Accuracy calculation
    testing_accuracy = accuracy_score(true_labels_test, ypred)
    # F1 score calculation
    testing_f1_score = f1_score(true_labels_test, ypred)
    return testing_accuracy, testing_sensitivity, testing_f1_score, cm

dfs[65]['class'] = dfs[65]['class'].astype(int)

from sklearn.ensemble import AdaBoostClassifier
from sklearn.preprocessing import StandardScaler

# Define the fraction of data to be used for the train set
train_fraction = 0.72 # 72% of the data will be used for training

# Set a fixed random seed for reproducibility
np.random.seed(42)

# Split the dataset into train and test sets
dfs_train, dfs_test = train_test_split(dfs, train_size=train_fraction, random_state=42)

# Concatenate the DataFrames in dfs_train to create the full training dataset
full_train_df = pd.concat(dfs_train)

# Prepare features (X_train_full) and target variable (y_train_full) for the full training dataset
X_train_full = full_train_df.drop(columns=['class'])
y_train_full = full_train_df['class']

# Feature Scaling
scaler_ada = StandardScaler()
X_train_scaled = scaler_ada.fit_transform(X_train_full)

# Use fixed optimal hyperparameters for AdaBoost
print("\n" + "="*50)
print("AdaBoost Classifier Results (with Feature Scaling)")
print("="*50)
best_params = {'n_estimators': 320, 'learning_rate': 0.73299}
print("Using hyperparameters:", best_params)

# Initialize and train AdaBoost with optimal parameters on scaled data
best_model = AdaBoostClassifier(n_estimators=320, learning_rate=0.73299, random_state=42)
best_model.fit(X_train_scaled, y_train_full)

# Calculate training accuracy (with scaling)
def training_calculation_scaled(model, dfs_train, scaler):
    ypred_train = []
    true_labels_train = []
    for df in dfs_train:
        X_train = df.drop(columns=['class'])
        X_train_sc = scaler.transform(X_train)
        y_pred_train = model.predict(X_train_sc)
        true_label = df['class'].iloc[0]
        true_labels_train.append(true_label)
        count_1s = sum(y_pred_train)
        if count_1s > 50:
            ypred_train.append(1)
        else:
            ypred_train.append(0)
    training_accuracy = accuracy_score(true_labels_train, ypred_train)
    return training_accuracy

def testing_calculation_scaled(model, dfs_test, scaler):
    true_labels_test = []
    ypred = []
    for df in dfs_test:
        X_test = df.drop(columns=['class'])
        X_test_sc = scaler.transform(X_test)
        y_pred_test = model.predict(X_test_sc)
        true_label = df['class'].iloc[0]
        true_labels_test.append(true_label)
        count_1s = sum(y_pred_test)
        if count_1s > 50:
            ypred.append(1)
        else:
            ypred.append(0)
    cm = confusion_matrix(true_labels_test, ypred)
    TN, FP, FN, TP = cm.ravel()
    testing_sensitivity = TP / (TP + FN)
    testing_accuracy = accuracy_score(true_labels_test, ypred)
    testing_f1_score = f1_score(true_labels_test, ypred)
    return testing_accuracy, testing_sensitivity, testing_f1_score, cm

training_accuracy = training_calculation_scaled(best_model, dfs_train, scaler_ada)
print("Training Accuracy:", training_accuracy)

accuracy, sensitivity, f1score, conf_matrix1 = testing_calculation_scaled(best_model, dfs_test, scaler_ada)
print("Testing Accuracy:", accuracy)
print("Testing Sensitivity:", sensitivity)
print("Testing F1 Score:", f1score)


#ROC Curve Calculation

true_labels_test = []
prediction_scores = []

for df in dfs_test:
    X_test = df.drop(columns=['class'])
    X_test_scaled = scaler_ada.transform(X_test)
    y_pred_proba = best_model.predict_proba(X_test_scaled)[:, 1]
    true_label = df['class'].iloc[0]
    true_labels_test.append(true_label)
    # Calculate prediction score as proportion of samples predicted as class 1
    avg_proba = np.mean(y_pred_proba)
    prediction_scores.append(avg_proba)

# Calculate ROC curve
fpr, tpr, thresholds = roc_curve(true_labels_test, prediction_scores)
roc_auc = auc(fpr, tpr)

# Plot ROC curve
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='red', lw=2, linestyle='--', label='Random Classifier')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate', fontsize=12)
plt.ylabel('True Positive Rate', fontsize=12)
plt.title('Receiver Operating Characteristic (ROC) Curve', fontsize=14)
plt.legend(loc="lower right", fontsize=10)
plt.grid(alpha=0.3)
plt.tight_layout()
plt.savefig('/Users/krishnayadav/Documents/test_projects/schizophrenia-journal/krishna/base/result/adaboost_roc_curve.png', dpi=150)
plt.close()
print("Saved: result/adaboost_roc_curve.png")

print(f"\nROC AUC Score: {roc_auc:.4f}")

# Plot confusion matrix
plt.figure(figsize=(6, 5))
sns.heatmap(conf_matrix1, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix - AdaBoost')
plt.savefig('/Users/krishnayadav/Documents/test_projects/schizophrenia-journal/krishna/base/result/adaboost_confusion_matrix.png', dpi=150)
plt.close()
print("Saved: result/adaboost_confusion_matrix.png")

"""# Adaboost results with new range - loguniform



# 20 samples in test

(1) 8 fold

Training Accuracy: 0.86
Best parameters: {'n_estimators': 251, 'learning_rate': np.float64(0.8991909447343751)}
Average validation accuracy with best parameters: 0.8452380952380952
Testing Accuracy: 0.85
Testing Sensitivity: 0.9166666666666666
Testing F1 Score: 0.88



(2) 9 fold

Training Accuracy: 0.88
Best parameters: {'n_estimators': 77, 'learning_rate': np.float64(0.7065294973103892)}
Average validation accuracy with best parameters: 0.8407407407407408
Testing Accuracy: 0.85
Testing Sensitivity: 1.0
Testing F1 Score: 0.8888888888888888

# Randomforest result for :

# 20 samples in test:

(1) 6 fold

Training Accuracy: 0.9
Best parameters: {'n_estimators': 90, 'max_depth': 6, 'min_samples_split': 16, 'min_samples_leaf': 13}
Average validation accuracy with best parameters: 0.8402777777777777
Testing Accuracy: 0.85
Testing Sensitivity: 0.9166666666666666
Testing F1 Score: 0.88


(2) 9 fold

Training Accuracy: 0.9
Best parameters: {'n_estimators': 393, 'max_depth': 5, 'min_samples_split': 7, 'min_samples_leaf': 2}
Average validation accuracy with best parameters: 0.8592592592592593
Testing Accuracy: 0.85
Testing Sensitivity: 0.9166666666666666
Testing F1 Score: 0.88

(3) 10 fold

Training Accuracy: 0.9
Best parameters: {'n_estimators': 152, 'max_depth': 5, 'min_samples_split': 16, 'min_samples_leaf': 11}
Average validation accuracy with best parameters: 0.8400000000000001
Testing Accuracy: 0.85
Testing Sensitivity: 0.9166666666666666
Testing F1 Score: 0.88
"""

from sklearn.ensemble import RandomForestClassifier
import numpy as np

# Set a fixed random seed for reproducibility
np.random.seed(42)

# Define the fraction of data to be used for the train set
train_fraction = 0.72    #72% of the data will be used for training

# Split the dataset into train and test sets
dfs_train, dfs_test = train_test_split(dfs, train_size=train_fraction, random_state=42)

# Concatenate the DataFrames in dfs_train to create the full training dataset
full_train_df = pd.concat(dfs_train)

# Prepare features (X_train_full) and target variable (y_train_full) for the full training dataset
X_train_full = full_train_df.drop(columns=['class'])
y_train_full = full_train_df['class']

# Feature Scaling for Random Forest
scaler_rf = StandardScaler()
X_train_rf_scaled = scaler_rf.fit_transform(X_train_full)

# Use fixed optimal hyperparameters for RandomForest
print("\n" + "="*50)
print("Random Forest Classifier Results (with Feature Scaling)")
print("="*50)
best_params = {'n_estimators': 152, 'max_depth': 5, 'min_samples_split': 16, 'min_samples_leaf': 11}
print("Using hyperparameters:", best_params)

# Initialize and train RandomForest with optimal parameters on scaled data
best_model = RandomForestClassifier(
    n_estimators=152,
    max_depth=5,
    min_samples_split=16,
    min_samples_leaf=11,
    random_state=42,
    n_jobs=-1
)
best_model.fit(X_train_rf_scaled, y_train_full)

# Calculate training accuracy
training_accuracy = training_calculation_scaled(best_model, dfs_train, scaler_rf)
print("Training Accuracy:", training_accuracy)

accuracy, sensitivity, f1score, conf_matrix2 = testing_calculation_scaled(best_model, dfs_test, scaler_rf)
print("Testing Accuracy:", accuracy)
print("Testing Sensitivity:", sensitivity)
print("Testing F1 Score:", f1score)
# Plot confusion matrix
plt.figure(figsize=(6, 5))
sns.heatmap(conf_matrix2, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix - RandomForest')
plt.savefig('/Users/krishnayadav/Documents/test_projects/schizophrenia-journal/krishna/base/result/randomforest_confusion_matrix.png', dpi=150)
plt.close()
print("Saved: result/randomforest_confusion_matrix.png")


#ROC Curve Calculation
true_labels_test = []
prediction_scores = []


for df in dfs_test:
    X_test = df.drop(columns=['class'])
    X_test_scaled = scaler_rf.transform(X_test)
    y_pred_proba = best_model.predict_proba(X_test_scaled)[:, 1]
    true_label = df['class'].iloc[0]
    true_labels_test.append(true_label)

    # Calculate prediction score as proportion of samples predicted as class 1
    avg_proba = np.mean(y_pred_proba)
    prediction_scores.append(avg_proba)


# Calculate ROC curve
fpr, tpr, thresholds = roc_curve(true_labels_test, prediction_scores)
roc_auc = auc(fpr, tpr)


# Plot ROC curve
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='red', lw=2, linestyle='--', label='Random Classifier')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate', fontsize=12)
plt.ylabel('True Positive Rate', fontsize=12)
plt.title('Receiver Operating Characteristic (ROC) Curve - Random Forest', fontsize=14)
plt.legend(loc="lower right", fontsize=10)
plt.grid(alpha=0.3)
plt.tight_layout()
plt.savefig('/Users/krishnayadav/Documents/test_projects/schizophrenia-journal/krishna/base/result/randomforest_roc_curve.png', dpi=150)
plt.close()
print("Saved: result/randomforest_roc_curve.png")
print(f"\nROC AUC Score: {roc_auc:.4f}")

"""# Improved Blending/Stacking with K-Fold Cross-Validation"""

from sklearn.model_selection import StratifiedKFold
from sklearn.base import clone

print("\n" + "="*60)
print("IMPROVED BLENDING/STACKING ENSEMBLE")
print("="*60)

# Split the dataset into train and test sets
train_dfs, test_dfs = train_test_split(dfs, test_size=20, random_state=42)

# Prepare full training data from all training subjects
full_train_df = pd.concat(train_dfs)
X_train_all = full_train_df.drop(columns=['class'])
y_train_all = full_train_df['class'].reset_index(drop=True)

# Feature scaling for blending
scaler_blend = StandardScaler()
X_train_blend_scaled = scaler_blend.fit_transform(X_train_all)

# Define base models with OPTIMIZED hyperparameters and class balancing
base_models = {
    'rf': RandomForestClassifier(n_estimators=152, max_depth=5, min_samples_split=16,
                                  min_samples_leaf=11, class_weight='balanced', random_state=42, n_jobs=-1),
    'ada': AdaBoostClassifier(n_estimators=320, learning_rate=0.73299, random_state=42),
    'gb': GradientBoostingClassifier(n_estimators=200, max_depth=5, random_state=42),
    'xgb': xgb.XGBClassifier(n_estimators=150, max_depth=5, learning_rate=0.1,
                              subsample=0.8, colsample_bytree=0.8, reg_lambda=1.0,
                              random_state=42, use_label_encoder=False, eval_metric='logloss')
}

print("Base models: RandomForest, AdaBoost, GradientBoosting, XGBoost")
print("Using K-Fold CV to generate out-of-fold predictions...")

# Generate out-of-fold predictions using Stratified K-Fold
n_splits = 5
kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)

# Store OOF predictions (PROBABILITIES instead of hard labels)
oof_preds = {name: np.zeros(len(y_train_all)) for name in base_models}

for fold_idx, (train_idx, val_idx) in enumerate(kfold.split(X_train_blend_scaled, y_train_all)):
    X_tr, X_val = X_train_blend_scaled[train_idx], X_train_blend_scaled[val_idx]
    y_tr = y_train_all.iloc[train_idx]

    for name, model in base_models.items():
        model_clone = clone(model)
        model_clone.fit(X_tr, y_tr)
        # Use PROBABILITIES instead of hard predictions
        oof_preds[name][val_idx] = model_clone.predict_proba(X_val)[:, 1]

print(f"Generated OOF predictions for {len(y_train_all)} samples (vs 25 samples before)")

# Create meta-features DataFrame from OOF predictions
meta_train = pd.DataFrame(oof_preds)
print(f"Meta-features shape: {meta_train.shape}")

# Train meta-model on ALL training data OOF predictions
gb_metamodel = GradientBoostingClassifier(n_estimators=100, max_depth=3, learning_rate=0.1, random_state=42)
gb_metamodel.fit(meta_train, y_train_all)
print("Meta-model trained: GradientBoostingClassifier")

# Train final base models on FULL scaled training data
print("Training final base models on full training data...")
final_models = {}
for name, model in base_models.items():
    model_clone = clone(model)
    model_clone.fit(X_train_blend_scaled, y_train_all)
    final_models[name] = model_clone

# Generate meta-features for TEST set (subject-level predictions)
print("\nGenerating predictions for test set...")
meta_test_list = []
l_true_test = []

for df in test_dfs:
    X_test = df.drop(columns=['class'])
    X_test_scaled = scaler_blend.transform(X_test)
    true_label = df['class'].iloc[0]
    l_true_test.append(true_label)

    # Get PROBABILITY predictions from each base model (average per subject)
    test_meta = {}
    for name, model in final_models.items():
        proba = model.predict_proba(X_test_scaled)[:, 1]
        test_meta[name] = np.mean(proba)  # Average probability for the subject

    meta_test_list.append(test_meta)

meta_test = pd.DataFrame(meta_test_list)
y_true = np.array(l_true_test)

# ============================================================
# TRAINING METRICS FOR BLENDING
# ============================================================
y_train_predict = gb_metamodel.predict(meta_train)
train_accuracy = accuracy_score(y_train_all, y_train_predict)

print("\n" + "-"*50)
print("BLENDING - Training Metrics:")
print("-"*50)
print(f"Training Accuracy: {train_accuracy:.4f}")

# ============================================================
# TEST METRICS FOR BLENDING
# ============================================================
# Predict using meta-model
y_pred_proba = gb_metamodel.predict_proba(meta_test)[:, 1]
y_pred_final = gb_metamodel.predict(meta_test)

# Calculate metrics
accuracy = accuracy_score(y_true, y_pred_final)
testing_f1_score = f1_score(y_true, y_pred_final)
conf_matrix = confusion_matrix(y_true, y_pred_final)

TP = conf_matrix[1][1]
FP = conf_matrix[0][1]
TN = conf_matrix[0][0]
FN = conf_matrix[1][0]

sensitivity = TP / (TP + FN) if (TP + FN) > 0 else 0
specificity = TN / (TN + FP) if (TN + FP) > 0 else 0
auc_roc = roc_auc_score(y_true, y_pred_proba)

print("\n" + "-"*50)
print("BLENDING - Test Metrics:")
print("-"*50)
print(f"Testing Accuracy: {accuracy:.4f}")
print(f"Testing Sensitivity: {sensitivity:.4f}")
print(f"Testing Specificity: {specificity:.4f}")
print(f"Testing F1 Score: {testing_f1_score:.4f}")
print(f"ROC AUC Score: {auc_roc:.4f}")

# Plot confusion matrix
plt.figure(figsize=(6, 5))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix - Improved Blending')
plt.savefig('/Users/krishnayadav/Documents/test_projects/schizophrenia-journal/krishna/base/result/blending_confusion_matrix.png', dpi=150)
plt.close()
print("Saved: result/blending_confusion_matrix.png")

# Plot ROC Curve
fpr, tpr, thresholds = roc_curve(y_true, y_pred_proba)

plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (AUC = {auc_roc:.4f})')
plt.plot([0, 1], [0, 1], color='red', lw=2, linestyle='--', label='Random Classifier')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate', fontsize=12)
plt.ylabel('True Positive Rate', fontsize=12)
plt.title('ROC Curve - Improved Blending', fontsize=14, fontweight='bold')
plt.legend(loc="lower right", fontsize=10)
plt.grid(alpha=0.3)
plt.tight_layout()
plt.savefig('/Users/krishnayadav/Documents/test_projects/schizophrenia-journal/krishna/base/result/blending_roc_curve.png', dpi=150)
plt.close()
print("Saved: result/blending_roc_curve.png")

# ============================================================
# FINAL SUMMARY
# ============================================================
print("\n")
print("="*60)
print("FINAL RESULTS SUMMARY")
print("="*60)
print("All models trained and evaluated!")
print("="*60)

# End of code

# -----------------------------
# XGBoost Classifier with Fixed Hyperparameters
# -----------------------------
print("\n" + "="*50)
print("XGBoost Classifier Results")
print("="*50)

np.random.seed(42)
train_fraction = 0.72
dfs_train, dfs_test = train_test_split(dfs, train_size=train_fraction, random_state=42)

full_train_df = pd.concat(dfs_train, ignore_index=True)
X_train_full = full_train_df.drop(columns=["class"])
y_train_full = full_train_df["class"]

best_params = {
    'n_estimators': 150,
    'max_depth': 5,
    'learning_rate': 0.1,
    'subsample': 0.8,
    'colsample_bytree': 0.8,
    'reg_lambda': 1.0
}
print("Using hyperparameters:", best_params)

best_model = xgb.XGBClassifier(
    **best_params,
    random_state=42,
    n_jobs=-1,
    objective="binary:logistic",
    eval_metric="logloss",
    tree_method="hist"
)
best_model.fit(X_train_full, y_train_full)

# -----------------------------
# Train metrics
# -----------------------------
training_accuracy = training_calculation(best_model, dfs_train)
print("Training Accuracy:", training_accuracy)

# -----------------------------
# Test metrics (your function)
# -----------------------------
accuracy, sensitivity, f1score, conf_matrix2 = testing_calculation(best_model, dfs_test)
print("Testing Accuracy:", accuracy)
print("Testing Sensitivity:", sensitivity)
print("Testing F1 Score:", f1score)

# Confusion Matrix
plt.figure(figsize=(6, 5))
sns.heatmap(conf_matrix2, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

# -----------------------------
# ROC Curve (subject-level score aggregation, same as yours)
# -----------------------------
true_labels_test = []
prediction_scores = []

for df in dfs_test:
    X_test = df.drop(columns=["class"])
    y_pred_proba = best_model.predict_proba(X_test)[:, 1]
    true_label = df["class"].iloc[0]

    true_labels_test.append(true_label)

    # Score per subject = mean predicted probability across that subject's samples
    prediction_scores.append(np.mean(y_pred_proba))

fpr, tpr, thresholds = roc_curve(true_labels_test, prediction_scores)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='red', lw=2, linestyle='--', label='Random Classifier')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate', fontsize=12)
plt.ylabel('True Positive Rate', fontsize=12)
plt.title('Receiver Operating Characteristic (ROC) Curve', fontsize=14)
plt.legend(loc="lower right", fontsize=10)
plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()

print(f"\nROC AUC Score: {roc_auc:.4f}")